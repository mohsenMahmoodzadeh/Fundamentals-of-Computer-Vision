{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "from skimage.metrics._structural_similarity import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readInputImages():\n",
    "    brussels1 = cv.imread('../Input/Brussels_1.jpg')\n",
    "    brussels2 = cv.imread('../Input/Brussels_2.jpg')\n",
    "    input_brussels = [brussels1, brussels2]\n",
    "    \n",
    "    mountain1 = cv.imread('../Input/Mountain_1.jpg')\n",
    "    mountain2 = cv.imread('../Input/Mountain_2.jpg')\n",
    "    mountain3 = cv.imread('../Input/Mountain_3.jpg')\n",
    "    mountain4 = cv.imread('../Input/Mountain_4.jpg')\n",
    "    mountain5 = cv.imread('../Input/Mountain_5.jpg')\n",
    "    mountain6 = cv.imread('../Input/Mountain_6.jpg')\n",
    "    mountain7 = cv.imread('../Input/Mountain_7.jpg')\n",
    "    input_mountain = [mountain6, mountain7, mountain5, mountain4, mountain1, mountain3, mountain2]\n",
    "    \n",
    "    office0 = cv.imread('../Input/office-00.jpg')\n",
    "    office1 = cv.imread('../Input/office-01.jpg')\n",
    "    office2 = cv.imread('../Input/office-02.jpg')\n",
    "    office3 = cv.imread('../Input/office-03.jpg')\n",
    "    input_office = [office3, office1, office2, office0]\n",
    "    \n",
    "    tower1 = cv.imread('../Input/Tower_1.jpg')\n",
    "    tower2 = cv.imread('../Input/Tower_2.jpg')\n",
    "    input_tower = [tower1, tower2]\n",
    "    \n",
    "    yard1 = cv.imread('../Input/yard-01.jpg')\n",
    "    yard2 = cv.imread('../Input/yard-02.jpg')\n",
    "    yard3 = cv.imread('../Input/yard-03.jpg')\n",
    "    yard4 = cv.imread('../Input/yard-04.jpg')\n",
    "    yard5 = cv.imread('../Input/yard-05.jpg')\n",
    "    yard6 = cv.imread('../Input/yard-06.jpg')\n",
    "    yard7 = cv.imread('../Input/yard-07.jpg')\n",
    "    yard8 = cv.imread('../Input/yard-08.jpg')\n",
    "    input_yard = [yard1, yard2, yard3, yard4, yard5, yard6, yard7, yard8]\n",
    "    \n",
    "    inputs = [input_brussels, input_mountain, input_office, input_tower, input_yard]\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "inputs = readInputImages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readReferenceImages():\n",
    "    brusselsRef = cv.imread('../Reference/Brussels_Ref.jpg')\n",
    "    mountainRef = cv.imread('../Reference/Mountain_Ref.jpg')\n",
    "    officeRef = cv.imread('../Reference/office_Ref.jpg')\n",
    "    towerRef = cv.imread('../Reference/Tower_Ref.jpg')\n",
    "    yardRef = cv.imread('../Reference/yard_Ref.jpg')\n",
    "    \n",
    "    references = [brusselsRef, mountainRef, officeRef, towerRef, yardRef]\n",
    "    \n",
    "    return references\n",
    "\n",
    "references = readReferenceImages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_homography(image1, image2):\n",
    "    \n",
    "    # detectAndCompute() detects keypoints and computes the descriptors.\n",
    "    keyPoints1, descriptor1 = sift.detectAndCompute(image1, None)\n",
    "    keyPoints2, descriptor2 = sift.detectAndCompute(image2, None)\n",
    "    \n",
    "    # Brute Force Matcher.knnMatch Finds the k best matches for each descriptor(basesd on a distance measure) from a query set.\n",
    "    # NORM_L2 or Eculidean distance is a distance meaure. \n",
    "    bfMatcher = cv.BFMatcher(cv.NORM_L2)\n",
    "    allMatchedPoints = bfMatcher.knnMatch(descriptor1, descriptor2, k=2)\n",
    "\n",
    "    ratio = 0.7\n",
    "    qualifiedMatchedPoints = []\n",
    "    for matchedPoint1, matchedPoint2 in allMatchedPoints:\n",
    "        if matchedPoint1.distance < ratio * matchedPoint2.distance:\n",
    "            qualifiedMatchedPoints.append(matchedPoint1)\n",
    "            \n",
    "    # Extract location of good matches        \n",
    "    keyPoints1 = np.float32([kp.pt for kp in keyPoints1])\n",
    "    keyPoints2 = np.float32([kp.pt for kp in keyPoints2])\n",
    "    \n",
    "    points1 = np.float32([keyPoints1[m.queryIdx] for m in qualifiedMatchedPoints])\n",
    "    points2 = np.float32([keyPoints2[m.trainIdx] for m in qualifiedMatchedPoints])\n",
    "    \n",
    "    # Finds a perspective transformation between two planes.(we can also use getPerspectiveTransform())\n",
    "    H, _ = cv.findHomography(points1, points2, cv.RANSAC)\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features(image1, image2, H):\n",
    "    height2, width2 = image2.shape[:2]\n",
    "    height1, width1 = image1.shape[:2]\n",
    "    \n",
    "    pts2 = np.float32([[0, 0], [0, height2], [width2, height2], [width2, 0]]).reshape(-1, 1, 2)\n",
    "    pts1 = np.float32([[0, 0], [0, height1], [width1, height1], [width1, 0]]).reshape(-1, 1, 2)\n",
    "    \n",
    "    # perspectiveTransform() Performs the perspective matrix transformation of vectors.\n",
    "    pts1_ = cv.perspectiveTransform(pts1, H)\n",
    "    # np.concatenate() Join a sequence of arrays along an existing axis(y-axis here).\n",
    "    pts = np.concatenate((pts2, pts1_), axis=0)\n",
    "    # np.ravel() Return a contiguous flattened array.\n",
    "    [xmin, ymin] = np.int32(pts.min(axis=0).ravel() - 0.5)\n",
    "    [xmax, ymax] = np.int32(pts.max(axis=0).ravel() + 0.5)\n",
    "    t = [-xmin, -ymin]\n",
    "    Ht = np.array([[1, 0, t[0]], [0, 1, t[1]], [0, 0, 1]])\n",
    "    \n",
    "    # warpPerspective() Applies a perspective transformation to an image.\n",
    "    result = cv.warpPerspective(image1, Ht.dot(H), (xmax - xmin, ymax - ymin))\n",
    "    result[t[1]:height2 + t[1], t[0]:width2 + t[0]] = image2\n",
    "    \n",
    "    # if src > thresh: maxvalue, else: 0\n",
    "    thresh = cv.threshold(cv.cvtColor(result, cv.COLOR_BGR2GRAY), 0, 255, cv.THRESH_BINARY)[1]\n",
    "    # Finds contours in a binary image. The contours are a useful tool for shape analysis and object detection and recognition.\n",
    "    # CHAIN_APPROX_SIMPLE compresses horizontal, vertical, and diagonal segments and leaves only their end points. For example, an up-right rectangular contour is encoded with 4 points.\n",
    "    # RETR_EXTERNAL retrieves only the extreme outer contours\n",
    "    contours = cv.findContours(thresh.copy(), cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    contours = imutils.grab_contours(contours)\n",
    "    max_area = max(contours, key=cv.contourArea)\n",
    "    # Calculates the up-right bounding rectangle of a point set.\n",
    "    (x, y, w, h) = cv.boundingRect(max_area)\n",
    "    result = result[y:y + h, x:x + w]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_images(images):\n",
    "    sift = cv.xfeatures2d.SIFT_create()\n",
    "    img0 = images[0]\n",
    "    stitched_image = []\n",
    "    for i in range(1, len(images)):\n",
    "        img_i = images[i]\n",
    "\n",
    "        img0_gray = cv.cvtColor(img0, cv.COLOR_BGR2GRAY)\n",
    "        img_i_gray = cv.cvtColor(img_i, cv.COLOR_BGR2GRAY)\n",
    "        \n",
    "        \n",
    "        H = get_Homography(img0_gray, img_i_gray)\n",
    "        img0 = match_features(img0, img_i, H)\n",
    "    \n",
    "    return img0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28069252383700166\n",
      "0.3446966639875461\n",
      "0.24324803536187845\n",
      "0.32353772326899105\n",
      "0.1355560023536538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final =stitch_images([inputs[0]])\n",
    "final = cv.resize(final[0], (references[0].shape[1], references[0].shape[0]))\n",
    "cv.imwrite('./brussels.jpg', final)\n",
    "print(ssim(final, references[0],  multichannel=True))\n",
    "\n",
    "final = stitch_images([inputs[1]])\n",
    "final = cv.resize(final[0], (references[1].shape[1], references[1].shape[0]))\n",
    "print(ssim(final, references[1],  multichannel=True))\n",
    "cv.imwrite('./mountain.jpg', final)\n",
    "\n",
    "final = stitch_images([inputs[2]])\n",
    "final = cv.resize(final[0], (references[2].shape[1], references[2].shape[0]))\n",
    "print(ssim(final, references[2],  multichannel=True))\n",
    "cv.imwrite('./office.jpg', final)\n",
    "\n",
    "final = stitch_images([inputs[3]])\n",
    "final = cv.resize(final[0], (references[3].shape[1], references[3].shape[0]))\n",
    "print(ssim(final, references[3],  multichannel=True))\n",
    "cv.imwrite('./tower.jpg', final)\n",
    "\n",
    "final = stitch_images([inputs[4]])\n",
    "final = cv.resize(final[0], (references[4].shape[1], references[4].shape[0]))\n",
    "print(ssim(final, references[4],  multichannel=True))\n",
    "cv.imwrite('./yard.jpg', final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
